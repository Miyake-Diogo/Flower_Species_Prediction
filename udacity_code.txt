# Load your model to this variable
import torch
from torch import nn
import torch.nn.functional as F
from torchvision import models
from collections import OrderedDict


    
model = models.densenet121(pretrained=True)
for param in model.parameters():
    param.requires_grad = False
classifier = nn.Sequential(OrderedDict([
                               ('fc1', nn.Linear(1024,512)),
                               ('relu', nn.ReLU()),
                               ('dropout1', nn.Dropout(0.1)),
                               ('fc2', nn.Linear(512,256)),
                               ('dropout2', nn.Dropout(0.1)),
                               ('relu', nn.ReLU()),
                               ('fc3', nn.Linear(256,102)),
                               ('output', nn.LogSoftmax(dim=1)) 
                               ]))

model.classifier = classifier

def load_checkpoint(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)
    
    model = models.densenet121(pretrained=True)
    
    classifier = nn.Sequential(OrderedDict([
                               ('fc1', nn.Linear(1024,512)),
                               ('relu', nn.ReLU()),
                               ('dropout1', nn.Dropout(0.5)),
                               ('fc2', nn.Linear(512,256)),
                               ('dropout2', nn.Dropout(0.1)),
                               ('relu', nn.ReLU()),
                               ('fc3', nn.Linear(256,102)),
                               ('output', nn.LogSoftmax(dim=1)) 
                               ]))
    
    model.classifier = classifier
    
    for param in model.parameters():
        param.requires_grad = False

    model.load_state_dict(checkpoint['state_dict'], strict = False)

    return model


model = load_checkpoint('/home/workspace/model_checkpoint.pth')
   
# If you used something other than 224x224 cropped images, set the correct size here
image_size = 224
# Values you used for normalizing the images. Default here are for 
# pretrained models from torchvision.
norm_mean = [0.485, 0.456, 0.406]
norm_std = [0.229, 0.224, 0.225]